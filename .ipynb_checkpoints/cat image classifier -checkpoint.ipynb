{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    train_data = h5py.File(\"datasets/train.h5\", \"r\") # read h5py file\n",
    "    train_x = np.array(train_data[\"train_set_x\"][:]) # extract training examples\n",
    "    train_y = np.array(train_data[\"train_set_y\"][:]) # extract training labels\n",
    "    \n",
    "    test_data = h5py.File(\"datasets/test.h5\", \"r\") # read h5py file\n",
    "    test_x = np.array(test_data[\"test_set_x\"][:])\n",
    "    test_y = np.array(test_data[\"test_set_y\"][:])\n",
    "    \n",
    "    classes = np.array(test_data[\"list_classes\"][:]) # test_data classes [\"cat\", \"not-cat\"]\n",
    "    \n",
    "    train_y = train_y.reshape((1, train_y.shape[0])) # reshape into vector\n",
    "    test_y = test_y.reshape((1, test_y.shape[0])) # reshape into vector\n",
    "    \n",
    "    return train_x, train_y, test_x, test_y, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_x_orig , train_y_orig , test_x_orig , test_y_orig, classes = load_data()\n",
    "\n",
    "# reshape training and test examples\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_parameters(layer_dims):\n",
    "    \n",
    "    # layer_dims is vector of size (n,1)\n",
    "    \n",
    "    parameters = {} # store weights and bias\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1,L):    \n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) * 0.01\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_sigmoid(z):\n",
    "    d_sig = (1 + np.exp(-z)) ** -1\n",
    "    return d_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):   \n",
    "    A = np.maximum(0,z)\n",
    "    cache = z\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = W.dot(A) + b\n",
    "    cache = (A, W, b)\n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward_activation(A_prev, W,  b, activation):\n",
    "    \n",
    "    if activation == \"relu\" or activation == \"RELU\":\n",
    "        z, cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(z)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        z, cache = linear_forward(A_prev, W, b)\n",
    "        A,activation_cache = sigmoid(z)\n",
    "        \n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache_ = (cache, activation_cache)\n",
    "    \n",
    "    return A, cache_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_layer(X, parameters):\n",
    "    \n",
    "    caches = [] # keep track of cost\n",
    "    A = X\n",
    "    L = len(parameters)//2\n",
    "\n",
    "    for l in range(1,L):\n",
    "        \n",
    "        A_prev = A # output of previous layer\n",
    "        A, cache = linear_forward_activation(A_prev, parameters[\"W\" + str(l)], parameters[\"b\" + str(l)], activation=\"relu\")\n",
    "        caches.append(cache)\n",
    "        \n",
    "    AL, cache = linear_forward_activation(A, parameters[\"W\" + str(L)], parameters[\"b\" + str(L)],activation=\"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    #print(AL.shape)\n",
    "    #print(X.shape[1])\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "    \n",
    "    return AL,caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = (1./m) * (-np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T))\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    \n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)\n",
    "    db = 1./m * np.sum(dZ, axis = 1, keepdims = True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev,dW, db, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward_activation(dA, cache, activation):\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's do backward propogation\n",
    "def backward_layer(AL, Y, cache):\n",
    "    \n",
    "    grads = {} # store derivatives of weights\n",
    "    L = len(cache) # number of layers\n",
    "    m = AL.shape[1] # number of training examples\n",
    "    Y = Y.reshape(AL.shape) \n",
    "    \n",
    "    # initialize dervative \n",
    "    dAL = -(np.divide(Y,AL) - np.divide(1-Y,1-AL))\n",
    "    \n",
    "    current_cache = cache[L-1]\n",
    "    \n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L-1)], grads[\"db\" + str(L-1)] = linear_backward_activation(dAL, current_cache,\n",
    "                                                                                             activation = \"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = cache[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_backward_activation(grads[\"dA\" + str(l + 1)], \n",
    "                                                                    current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(L):\n",
    "        print(l)\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layer_dims, learning_rate=0.0075, epochs=3000, print_cost=False):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    \n",
    "    # parameter initialization\n",
    "    parameters = initialization_parameters(layer_dims)\n",
    "\n",
    "    for i in (1,epochs):\n",
    "\n",
    "        AL, caches = forward_layer(X,parameters)\n",
    "        \n",
    "        cost = compute_cost(AL,Y)\n",
    "        \n",
    "        grads = backward_layer(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 100 == 0 or i == epochs - 1:\n",
    "            print(\"cost after iteration {} {} \".format(i, np.squeeze(cost)))\n",
    "        if i % 100 or i == epochs:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    return parameters , cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W1': array([[ 0.01624345, -0.00611756, -0.00528172, ..., -0.00527214,\n",
      "        -0.0038034 ,  0.00949412],\n",
      "       [ 0.01009231,  0.00229889, -0.00664099, ...,  0.00689859,\n",
      "        -0.00488322,  0.0020761 ],\n",
      "       [-0.0035634 , -0.00195481,  0.00636803, ...,  0.00822751,\n",
      "        -0.00104425, -0.00657957],\n",
      "       ...,\n",
      "       [ 0.00174745, -0.00130162,  0.01835827, ..., -0.00922606,\n",
      "        -0.00824792, -0.00153355],\n",
      "       [-0.0003495 , -0.00417018, -0.0085517 , ...,  0.02247331,\n",
      "        -0.00533637, -0.00029554],\n",
      "       [-0.01434299, -0.01110641,  0.00726317, ...,  0.0219863 ,\n",
      "         0.01538192,  0.00746604]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[ 9.06010848e-03, -1.37984045e-02, -1.68807982e-02,\n",
      "        -1.79974771e-03,  2.65648369e-03,  5.51618105e-03,\n",
      "         1.37907609e-02,  2.12982825e-03,  2.37150021e-03,\n",
      "         6.03240968e-03,  9.68538642e-03, -2.83641403e-02,\n",
      "        -6.97473387e-03,  4.55800593e-03, -6.70228698e-03,\n",
      "         2.37748871e-03,  5.11456469e-03,  1.88228427e-02,\n",
      "         4.85457191e-03, -9.76899745e-03],\n",
      "       [ 1.08638374e-02, -4.14613832e-03,  1.10164656e-02,\n",
      "         1.07644714e-02, -3.31802917e-03, -1.40468582e-02,\n",
      "        -1.06350209e-02,  1.72567493e-02, -2.73935212e-03,\n",
      "         1.64070108e-02, -1.37523725e-03,  2.26952545e-03,\n",
      "         4.09665707e-05,  1.35503053e-02,  4.57240914e-03,\n",
      "        -2.26461285e-02, -5.64168714e-03,  7.96791462e-03,\n",
      "         5.71924352e-03, -2.25757130e-03],\n",
      "       [ 1.10472683e-02,  8.25821871e-03, -3.78917415e-03,\n",
      "         3.05093903e-04,  1.17623653e-02, -7.51034065e-03,\n",
      "        -9.66242060e-04, -2.62975333e-03, -8.24782068e-03,\n",
      "        -3.76539346e-03, -2.22535734e-02,  2.19026373e-03,\n",
      "         2.38906750e-03,  1.30244534e-02, -8.33076572e-05,\n",
      "        -3.08112826e-03, -9.49104953e-03,  6.08211783e-03,\n",
      "         1.75981068e-02, -7.11977790e-03],\n",
      "       [ 5.94062552e-03, -1.12849271e-02,  1.73736049e-02,\n",
      "         7.82664020e-04, -2.65405269e-03, -1.68299644e-03,\n",
      "        -7.29697733e-03, -7.14995255e-03,  3.87583994e-04,\n",
      "        -5.72993495e-03,  1.72816681e-02, -1.52948813e-02,\n",
      "         9.34955837e-05, -1.67580951e-02, -4.79998567e-03,\n",
      "         6.83001942e-03,  1.41925034e-02,  8.41918550e-03,\n",
      "        -2.15210169e-03, -5.44027050e-03],\n",
      "       [-7.38978455e-03,  1.76740362e-02, -2.60153308e-02,\n",
      "         6.91981069e-03, -1.40095092e-02,  1.50809492e-03,\n",
      "        -1.16792842e-02,  1.19906294e-02,  2.13354613e-02,\n",
      "        -7.90825263e-03, -6.32338693e-03, -3.45369017e-03,\n",
      "         1.34916198e-04, -1.28500376e-02,  2.17659765e-03,\n",
      "        -3.41623658e-03,  6.55832653e-03,  3.49921131e-03,\n",
      "        -6.57882204e-03, -1.76247729e-02],\n",
      "       [-1.08923746e-02,  4.33584604e-03,  7.96994867e-03,\n",
      "         9.07592875e-03,  6.93928572e-03,  1.18549019e-02,\n",
      "        -1.25620781e-03, -1.27856801e-02,  1.49475232e-02,\n",
      "         2.24418461e-03, -1.04445443e-02,  2.26266925e-03,\n",
      "         7.84455881e-04, -1.64225705e-02,  7.46050333e-03,\n",
      "        -1.57298436e-02, -2.90022875e-03, -1.35819376e-02,\n",
      "         5.37919939e-03, -1.69885624e-02],\n",
      "       [ 9.25218130e-03, -1.04912075e-02, -1.81406168e-02,\n",
      "         2.65914682e-03,  4.10203522e-03, -1.46543520e-02,\n",
      "        -3.07742339e-03, -2.74315380e-02,  8.50614383e-03,\n",
      "        -6.37711842e-03,  2.52260006e-04, -5.89367837e-03,\n",
      "         4.17909026e-03, -8.98837764e-04,  4.18115270e-03,\n",
      "         1.72380344e-02,  1.48114366e-02, -7.89647207e-04,\n",
      "         4.15887005e-03, -5.03740626e-03]]), 'b2': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W3': array([[ 0.00553773,  0.00958623,  0.02292505, -0.02837509, -0.01693258,\n",
      "        -0.0046076 , -0.00415418],\n",
      "       [-0.01534476,  0.00737094, -0.00145589, -0.00365178,  0.00619384,\n",
      "         0.0198597 , -0.00913212],\n",
      "       [ 0.00423611,  0.00771669,  0.00024627,  0.0070192 , -0.00170321,\n",
      "         0.02053139, -0.00026629],\n",
      "       [ 0.01400803,  0.01434208,  0.02593699, -0.00144698,  0.01064807,\n",
      "         0.00013288, -0.00037595],\n",
      "       [-0.02559864, -0.00827045,  0.01389408, -0.004978  , -0.01040001,\n",
      "         0.00515843, -0.01557394]]), 'b3': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W4': array([[-0.00859177, -0.00044166,  0.01119846,  0.02007237,  0.0004181 ]]), 'b4': array([[0.]])}\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'dW4'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-b3d35b1dd7b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlayers_dims\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[1;33m[\u001b[0m\u001b[1;36m12288\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#  4-layer model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_y_orig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-149-696147c01317>\u001b[0m in \u001b[0;36mmodel\u001b[1;34m(X, Y, layer_dims, learning_rate, epochs, print_cost)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackward_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mupdate_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-141-0e219a41d6fe>\u001b[0m in \u001b[0;36mupdate_parameters\u001b[1;34m(parameters, grads, learning_rate)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"W\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dW\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"b\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"db\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'dW4'"
     ]
    }
   ],
   "source": [
    "layers_dims =  [12288, 20, 7, 5, 1] #  4-layer model\n",
    "parameters, cost = model(train_x,train_y_orig, layers_dims, learning_rate=0.01, epochs=2500, print_cost=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
