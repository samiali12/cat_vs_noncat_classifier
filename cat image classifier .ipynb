{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for computation numerical computations\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math\n",
    "\n",
    "# model visualization\n",
    "import matplotlib.pyplot as plt # for data visualization\n",
    "import gradio as gr # build model user interface\n",
    "\n",
    "# helper functions\n",
    "from utils import * # helper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_x_orig , train_y_orig , test_x_orig , test_y_orig, classes = load_data()\n",
    "\n",
    "# reshape training and test examples\n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialization_parameters(layer_dims):\n",
    "    \n",
    "    # layer_dims is vector of size (n,1)\n",
    "    \n",
    "    parameters = {} # store weights and bias\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1,L):\n",
    "        \n",
    "        # initialize weight for layers\n",
    "        parameters[\"W\" + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1]) / np.sqrt(layer_dims[l-1])\n",
    "        \n",
    "        # initialize bias for layers\n",
    "        parameters[\"b\" + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize parameters for adam gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize velocity\n",
    "    for l in range(1, L + 1):\n",
    "        v[\"dW\" + str(l)] = np.zeros_like(parameters[\"W\" + str(l)])\n",
    "        v[\"db\" + str(l)] = np.zeros_like(parameters[\"b\" + str(l)])\n",
    "        \n",
    "        s[\"dW\" + str(l)] = np.zeros_like(parameters[\"W\" + str(l)])\n",
    "        s[\"db\" + str(l)] = np.zeros_like(parameters[\"b\" + str(l)])\n",
    "\n",
    "    return v,s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate random batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_batch_normalize(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1, m))\n",
    "    \n",
    "    inc = mini_batch_size\n",
    "\n",
    "    # Step 2 - Partition (shuffled_X, shuffled_Y).\n",
    "    \n",
    "    num_complete_minibatches = math.floor(m / mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        mini_batch_X = shuffled_X[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:,k * mini_batch_size:(k + 1) * mini_batch_size]\n",
    "        \n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "    \n",
    "    # For handling the end case (last mini-batch < mini_batch_size i.e less than 64)\n",
    "    if m % mini_batch_size != 0:\n",
    "        end = m - mini_batch_size * math.floor(m / mini_batch_size)\n",
    "        mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n",
    "        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:]\n",
    "\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "        \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, s,v, t, beta1=0.9, beta2=0.999,\n",
    "                                learning_rate=0.01,epsilon = 1e-8):\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "\n",
    "        v[\"dW\" + str(l + 1)] = beta1 * v[\"dW\" + str(l + 1)] + (1 - beta1) * grads['dW' + str(l + 1)]\n",
    "        v[\"db\" + str(l + 1)] = beta1 * v[\"db\" + str(l + 1)] + (1 - beta1) * grads['db' + str(l + 1)]\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)] / (1 - np.power(beta1, t))\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        s[\"dW\" + str(l + 1)] = beta2 * s[\"dW\" + str(l + 1)] + (1 - beta2) * np.power(grads['dW' + str(l + 1)], 2)\n",
    "        s[\"db\" + str(l + 1)] = beta2 * s[\"db\" + str(l + 1)] + (1 - beta2) * np.power(grads['db' + str(l + 1)], 2)\n",
    "        \n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        s_corrected[\"dW\" + str(l + 1)] = s[\"dW\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)] / (1 - np.power(beta2, t))\n",
    "        \n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)] - learning_rate * v_corrected[\"dW\" + str(l + 1)] / np.sqrt(s_corrected[\"dW\" + str(l + 1)] + epsilon)\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)] - learning_rate * v_corrected[\"db\" + str(l + 1)] / np.sqrt(s_corrected[\"db\" + str(l + 1)] + epsilon)\n",
    "\n",
    "    return parameters, v, s, v_corrected, s_corrected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compute cost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_training_cost(AL, Y, parameters, lambd):\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    L = len(parameters)//2\n",
    "    parameters_sum = 0\n",
    "    \n",
    "    for l in range(0,L):\n",
    "        parameters_sum += np.sum(np.square(parameters[\"W\" + str(l+1)]))\n",
    "        \n",
    "    L2 = lambd * parameters_sum / (2*m)\n",
    "    \n",
    "    # Compute loss from aL and y.\n",
    "    cost = -np.dot(Y,np.log(AL).T) - np.dot(1-Y, np.log(1-AL).T)\n",
    "    \n",
    "    cost += L2\n",
    "    total_cost = np.sum(cost)\n",
    "\n",
    "    \n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layer_dims, batch_size = 64, beta1 = 0.9, beta2=0.999,\n",
    "          learning_rate=0.0075, lambd=0.7, epochs=3000, epsilon=1e-8, print_cost=False, batch_normalize=True):\n",
    "    \n",
    "    ''' our model function take all hyperparameter and merge all function\n",
    "    into one'''\n",
    "\n",
    "    costs = []\n",
    "    seed = 10\n",
    "    m = X.shape[1]\n",
    "    t=2\n",
    "   \n",
    "    \n",
    "    # parameter initialization\n",
    "    parameters = initialization_parameters(layer_dims) # weight initialization\n",
    "    s,v = initialize_adam(parameters)\n",
    "    \n",
    "    \n",
    "    for i in range(1,epochs):\n",
    "        \n",
    "        if batch_normalize == True:\n",
    "            \n",
    "            cost_total = 0\n",
    "            seed = seed + 1\n",
    "            mini_batches = mini_batch_normalize(X,Y,batch_size,seed)\n",
    "\n",
    "            for minibatch in mini_batches:\n",
    "        \n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "            \n",
    "                AL, caches = forward_layer(minibatch_X,parameters)\n",
    "        \n",
    "                cost_total += compute_training_cost(AL,minibatch_Y,parameters,lambd)\n",
    "        \n",
    "                grads = backward_layer(AL,minibatch_Y, caches, lambd)\n",
    "        \n",
    "                #parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, s, v,t,  beta1, beta2, learning_rate, epsilon)\n",
    "        \n",
    "                seed += 1\n",
    "                cost_avg = cost_total/m\n",
    "        \n",
    "                if print_cost and i % 1000 == 0:\n",
    "                    print(\"cost after iteration {} {} \".format(i, cost_avg))\n",
    "                if i % 100 == 0 and i == print_cost:\n",
    "                    costs.append(cost_total)\n",
    "                    \n",
    "        else:\n",
    "            AL, caches = forward_layer(X,parameters)\n",
    "        \n",
    "            cost = 1./m * compute_training_cost(AL,Y,parameters,lambd)\n",
    "        \n",
    "            grads = backward_layer(AL, Y, caches, lambd)\n",
    "        \n",
    "                #parameters = update_parameters(parameters, grads, learning_rate)\n",
    "            parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, s, v,t,  beta1, beta2, learning_rate, epsilon)\n",
    "        \n",
    "            if print_cost and i % 100 == 0:\n",
    "                print(\"cost after iteration {} {} \".format(i, cost))\n",
    "            if i % 100 == 0 and i == print_cost:\n",
    "                costs.append(cost_total)\n",
    "       \n",
    "    return parameters , costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims =  [12288, 20, 7, 5, 1] #  4-layer model\n",
    "parameters, cost = model(train_x,train_y_orig,layers_dims, epochs=2500, print_cost=True)\n",
    "\n",
    "p = predict(train_x,train_y_orig,parameters) # training acuracy\n",
    "pred_test = predict(test_x, test_y_orig, parameters) # test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "my_image = \"my_image.jpg\" # change this to the name of your image file \n",
    "my_label_y = [1] # the true class of your image (1 -> cat, 0 -> non-cat)\n",
    "\n",
    "num_px= 64\n",
    "fname = \"\" + my_image\n",
    "image = np.array(Image.open(fname).resize((64,64)))\n",
    "plt.imshow(image)\n",
    "image = image / 255.\n",
    "image = image.reshape((1, num_px * num_px * 3)).T\n",
    "\n",
    "my_predicted_image = predict(image, my_label_y, parameters)\n",
    "\n",
    "print (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
